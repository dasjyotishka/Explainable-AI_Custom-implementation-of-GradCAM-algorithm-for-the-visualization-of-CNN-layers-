{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasjyotishka/Explainable-AI_Custom-implementation-of-GradCAM-algorithm-for-the-visualization-of-CNN-layers-/blob/main/TP2_GradCAM_Jyotishka_Jialin_Zeli_Final_result.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rei4HUPHu21q"
      },
      "source": [
        "## Visualization of CNN: Grad-CAM\n",
        "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n",
        "\n",
        "\n",
        "* NB: if `PIL` is not installed, try `conda install pillow`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-X6Hww7u21w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import urllib.request\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xENJYP3au21y"
      },
      "source": [
        "### Download the Model\n",
        "We provide you a pretrained model `ResNet-34` for `ImageNet` classification dataset.\n",
        "* **ImageNet**: A large dataset of photographs with 1 000 classes.\n",
        "* **ResNet-34**: A deep architecture for image classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF--aWReu21z"
      },
      "outputs": [],
      "source": [
        "resnet34 = models.resnet34(pretrained=True)\n",
        "resnet34.eval() # set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zKF3pYhu21z"
      },
      "source": [
        "![ResNet34](https://miro.medium.com/max/1050/1*Y-u7dH4WC-dXyn9jOG4w0w.png)\n",
        "\n",
        "\n",
        "Input image must be of size (3x224x224).\n",
        "\n",
        "First convolution layer with maxpool.\n",
        "Then 4 ResNet blocks.\n",
        "\n",
        "Output of the last ResNet block is of size (512x7x7).\n",
        "\n",
        "Average pooling is applied to this layer to have a 1D array of 512 features fed to a linear layer that outputs 1000 values (one for each class). No softmax is present in this case. We have already the raw class score!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9Dkyiinu210"
      },
      "outputs": [],
      "source": [
        "classes = pickle.load(urllib.request.urlopen('https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'))\n",
        "\n",
        "#classes is a dictionary with the name of each class\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlejjUD9u211"
      },
      "source": [
        "### Input Images\n",
        "We provide you 20 images from ImageNet (download link on the webpage of the course or download directly using the following command line,).<br>\n",
        "In order to use the pretrained model resnet34, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnu6FSwsu212"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the images\n",
        "def preprocess_image(dir_path):\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    dataset = datasets.ImageFolder(dir_path, transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224), # resize the image to 224x224\n",
        "            transforms.ToTensor(), # convert numpy.array to tensor\n",
        "            normalize])) #normalize the tensor\n",
        "\n",
        "    return (dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfVPHQrBu214",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# The images should be in a *sub*-folder of \"data/\" (ex: data/TP2_images/images.jpg) and *not* directly in \"data/\"!\n",
        "# otherwise the function won't find them\n",
        "\n",
        "import os\n",
        "os.mkdir(\"data\")\n",
        "os.mkdir(\"data/TP2_images\")\n",
        "!cd data/TP2_images && wget \"https://www.lri.fr/~gcharpia/deeppractice/2023/TP2/TP2_images.zip\" && unzip TP2_images.zip\n",
        "dir_path = \"data/\"\n",
        "# Delete the zip file, which is not needed anymore\n",
        "os.remove('/content/data/TP2_images/TP2_images.zip')\n",
        "dataset = preprocess_image(dir_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omyintAAu215"
      },
      "outputs": [],
      "source": [
        "# Show the orignal image\n",
        "index = 5\n",
        "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
        "plt.imshow(input_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3qFNEr1u215"
      },
      "outputs": [],
      "source": [
        "output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n",
        "values, indices = torch.topk(output, 3)\n",
        "# To store the indices of the top three classes predicted\n",
        "top_3_classes = indices[0].numpy()\n",
        "# To store the labels of the top three classes predicted\n",
        "top_3_names= [classes[x] for x in indices[0].numpy()]\n",
        "print(\"Top 3-classes:\", top_3_classes , [classes[x] for x in indices[0].numpy()])\n",
        "print(\"Raw class scores:\", values[0].detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDzvs0GIu216"
      },
      "source": [
        "### Grad-CAM\n",
        "* **Overview:** Given an image, and a category (‘tiger cat’) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
        "\n",
        "\n",
        "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the given images. For each image, choose the top-3 possible labels as the desired classes. Compare the heatmaps of the three classes, and conclude.\n",
        "\n",
        "\n",
        "* **To be submitted within 2 weeks**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
        "\n",
        "\n",
        "* **Hints**:\n",
        " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully.\n",
        " + The pretrained model resnet34 doesn't have an activation function after its last layer, the output is indeed the `raw class scores`, you can use them directly.\n",
        " + The size of feature maps is 7x7, so your heatmap will have the same size. You need to project the heatmap to the resized image (224x224, not the original one, before the normalization) to have a better observation. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  \n",
        " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0ib0Pxzu216"
      },
      "source": [
        "Class: ‘pug, pug-dog’ | Class: ‘tabby, tabby cat’\n",
        "- | -\n",
        "![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog.jpg)| ![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/cat.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "\n",
        "# To get the parameters of the last fully-connected network in a list\n",
        "params = list(resnet34.fc.parameters())\n",
        "# The weights of the last fully-connected network are stored in a numpy array for easy indexing with the output classes.\n",
        "# The shape of weights is (1000, 512)\n",
        "weight = np.squeeze(params[0].data.numpy())\n",
        "print('weight.shape', weight.shape)\n",
        "\n",
        "\n",
        "# Custom GradCAM implementation function\n",
        "def return_CAM(feature_conv, weight, class_idx):\n",
        "  \"\"\"\n",
        "  return_CAM generates the CAMs and up-sample it to 224x224\n",
        "  arguments:\n",
        "  feature_conv: the feature maps of the last convolutional layer of the model\n",
        "  weight: the weights that have been extracted from the trained parameters\n",
        "  class_idx: the label of the class for which we are interested to generate the heatmap\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Since we only consider one input image at a time, therefore in this case, the shape of the image after the last convolution layer is (1, 512, 7, 7)\n",
        "  bz, nc, h, w = feature_conv.shape\n",
        "\n",
        "  # The heatmap should be upscaled to 224x224 to superimpose it on the original image\n",
        "  size_upsample = (224, 224)\n",
        "\n",
        "  output_cam = []\n",
        "\n",
        "  # The shape of beforeDot is (512, 49)\n",
        "  beforeDot =  feature_conv.reshape((nc, h*w))\n",
        "\n",
        "  # The weight corresponding to the desired label in the last fully connected network is multiplied with beforeDot\n",
        "  # This, in effect, gives us a featuremap, in which only the excitations with respect to the desired class is highlighted\n",
        "  # The shape of cam is (1, 512) x (512, 49) = (1, 49)\n",
        "  cam = np.matmul(weight[class_idx], beforeDot)\n",
        "\n",
        "  # The shape of cam is resized to (7, 7)\n",
        "  cam = cam.reshape(h, w)\n",
        "  # Scaling the pixel values corresponding to cam between 0 to 255\n",
        "  cam = cam - np.min(cam)\n",
        "  cam_img = cam / np.max(cam)\n",
        "  cam_img = np.uint8(255 * cam_img)\n",
        "  # Resizing the cam_img to the upscale value\n",
        "  output_cam.append(cv2.resize(cam_img, size_upsample))\n",
        "  return output_cam\n",
        "\n",
        "# Defining a hook function to get the feature maps of the last convolutional layer\n",
        "def hook_fn(module, input, output):\n",
        "    # Get the feature maps of the last convolutional layer\n",
        "    global feature_maps\n",
        "    feature_maps = output\n",
        "\n"
      ],
      "metadata": {
        "id": "9zK7-yfH2QKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YRUDSQjBHVr"
      },
      "outputs": [],
      "source": [
        "# There are 20 pictures in the test directory\n",
        "for index in range(20):\n",
        "\n",
        "  # Plot the original image\n",
        "  img = cv2.imread(dataset.imgs[index][0])\n",
        "  img = cv2.resize(img, (224,224))\n",
        "  rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)# Convert the image from BGR to RGB\n",
        "  rgb_img = np.float32(rgb_img)/255 # Convert the image to a float32 numpy array\n",
        "\n",
        "\n",
        "  # Plotting the original image\n",
        "  fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "  axs[0].imshow(rgb_img)\n",
        "  axs[0].set_title('Original Image')\n",
        "\n",
        "  image= dataset[index][0].view(1,3,224,224)\n",
        "  output = resnet34(image)\n",
        "  values, indices = torch.topk(output, 3)\n",
        "  # Getting the top 3 predicted class indices\n",
        "  top_3_classes = indices[0].numpy()\n",
        "  # Getting the top 3 predicted class names\n",
        "  top_3_names= [classes[x] for x in indices[0].numpy()]\n",
        "\n",
        "  # Attach the hook to the last convolutional layer\n",
        "  handle = resnet34.layer4[2].register_forward_hook(hook_fn)\n",
        "  # Pass an input image through the network to get the feature maps\n",
        "  resnet34(image)\n",
        "  # Detach the hook after getting the feature maps\n",
        "  handle.remove()\n",
        "\n",
        "\n",
        "  n=0\n",
        "  for i in top_3_classes:\n",
        "\n",
        "    # We have to specify the target(i) we want to generate the Class Activation Maps for.\n",
        "    grayscale_cam = return_CAM(feature_maps.detach().numpy(), weight, i) # generate the CAM for the input image\n",
        "    heatmap = cv2.applyColorMap(grayscale_cam[0], cv2.COLORMAP_JET) # generate the heatmap after processing the CAM\n",
        "    heatmap[:,:,0], heatmap[:,:,2] = heatmap[:,:,2], heatmap[:,:,0].copy()\n",
        "    #Projecting the image from the target layer on the input image after pre-processing (224x224x3)\n",
        "    # A small fraction of the heatmap colour gradient is superimposed on the original image\n",
        "    projection = rgb_img + heatmap*0.003\n",
        "    projection = projection - np.min(projection)\n",
        "    projection = projection / np.max(projection)\n",
        "    projection = np.uint8(255 * projection)\n",
        "    n+=1\n",
        "\n",
        "    # Plot figure\n",
        "    axs[n].imshow(projection)\n",
        "    axs[n].set_title(top_3_names[n-1])\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comments on the observation of our heatmaps**:\n",
        "\n",
        "\n",
        "We can see that our custom Grad-CAM algorithm can place red/yellow/blue circular highlight around the object relatively accurately. For example, in the 2nd row of the above image, all 3 heatmaps are able to apply red highlight on the animal, regardless of the label (porcupine, marmoset, sloth bear).\n",
        "\n",
        "\n",
        "In addition, our Grad-CAM algorithm can shift the focus towards the correct object depending on the class. For example, in the 6th row of the images above with two cats, the left cat appears to be an egyptian cat and the right cat appears to be a tiger cat based on our manual visual comparison of the photo with Google images. When the predicted label is \"Egyptian cat\", our heat map has red colour focused on the head of the left cat. When the predicted label is \"Tiger cat\", our heat map has red colour concentrated on the hip of the right cat, an area with distinct black and yellow strips of hair that makes the cat look like a tiger (we think this is where the name, \"Tiger cat\" comes from).\n",
        "\n",
        "\n",
        "Similarly, for the 8th row of the images above, with two dogs, the top dog appears to be a chesapeake bay retriever and the bottom dog appears to be a great dame based on our manual visual comparison of the photo with Google images. The heat map has red colour concentrated on the face of the bottom dog when it classifies the image as a great dame, while the red colour is concentrated on the face of the top dog when it classifies the image as a chesapeake bay retriever.\n",
        "\n",
        "These examples show that with the help of the heatmaps, we can be fairly sure that the model is able to classify the images by really focusing on the regions of interest, which helps in vouching for the explainability of the model."
      ],
      "metadata": {
        "id": "Xzqzd69VnCbB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}